{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzXXiqM-reDf",
        "colab_type": "text"
      },
      "source": [
        "## Building A ChatBot Using Deep NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke1PAElolLk8",
        "colab_type": "code",
        "outputId": "1de1a65e-8425-4113-c905-39f39d0437bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install tensorflow==1.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (46.3.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MrIoqUcryxo",
        "colab_type": "text"
      },
      "source": [
        "- We have downloaded the Cornell Movies Dataset.\n",
        "- After downloading, we will first create a difference between Metadata and the real training data.\n",
        "- Metadata is the discriptional data of movies like the ratings, the name of the movies from which the dialogues have been taken etc. We do not need that data to train our chatbot.\n",
        "- We only need the conversations between our characters. These are the only data we will be using.\n",
        "\n",
        "- We will only be using 2 files from the cornell movie folder\n",
        "1. movie_conversations.txt\n",
        "2. movie_lines.txt\n",
        "\n",
        "- The columns in movie_lines.txt are as follows:\n",
        "1. L0123 - This column represents line numbers\n",
        "2. U0, U1 - This column represents the characters in the movie as user0, user1\n",
        "3. m0, m1 - This column represents movies from which the line has been taken. movie0, movie1\n",
        "4. Bianca, Camroon - Names of the characters\n",
        "5. Last column is the conversations between the characters\n",
        "\n",
        "- The last column in movie_conversations.txt represents conversations in the form of line numbers given in movie_lines.txt.\n",
        "- This file helps us to seperate the conversations from one another which is not done in movie_lines.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0xgpTf_lPVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re  #used to clean the text and make it simple as possible for the chatbot to learn in the best conditions\n",
        "import time #to measure training time of each epoch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcJI726klgAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open('/content/drive/My Drive/CSV files for Deep learning and ML/movie_lines.txt', encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")\n",
        "conversations = open('/content/drive/My Drive/CSV files for Deep learning and ML/movie_conversations.txt', encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUz3gaKlPF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        " \n",
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        " \n",
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])\n",
        "\n",
        "contractions = {\"aight\" : \"alright\",\n",
        " \"ain't\": \"am not\",\n",
        " \"amn't\" : \"am not\",\n",
        " \"aren't\": \"are not\",\n",
        " \"can't\": \"cannot\",\n",
        " \"'cause\" : \"because\",\n",
        " \"could've\": \"could have\",\n",
        " \"couldn't\" : \"could not\",\n",
        " \"couldn't've\" : \"could not have\", \n",
        " \"daren't\" : \"dare not\",\n",
        " \"daresn't\" : \"dare not\",\n",
        " \"dasn't\" : \"dare not\",\n",
        " \"didn't\" : \"did not\",\n",
        " \"doesn't\" : \"does not\",\n",
        " \"don't\" : \"do not\",\n",
        " \"d'ye\" : \"do you\",\n",
        " \"e'er\" : \"ever\",\n",
        " \"everybody's\" : \"everybody is\",\n",
        " \"everyone's\" : \"everyone is\",\n",
        " \"finna\":\"fixing to\",\n",
        " \"g'day\" : \"good day\",\n",
        " \"gimme\" : \"give me\",\n",
        " \"giv'n\": \"given\",\n",
        " \"gonna\":\"going to\",\n",
        " \"gon't\":\"go not\",\n",
        " \"gotta\":\"got to\",\n",
        " \"hadn't\":\"had not\",\n",
        " \"had've\":\"had have\",\n",
        " \"hasn't\":\"has not\",\n",
        " \"haven't\":\"have not\",\n",
        " \"he'd\":\"he would\",\n",
        " \"he'dn't've'd\":\"he would not have had\",\n",
        " \"he'll\":\"he will\",\n",
        " \"he's\":\"he is\",\n",
        " \"he've\":\"he have\",\n",
        " \"how'd\":\"how did\",\n",
        " \"howdy\":\"how do you do\",\n",
        " \"how'll\":\"how will\",\n",
        " \"how're\":\"how are\",\n",
        " \"how's\":\"how has\",\n",
        " \"i'd\": \"I would\",\n",
        " \"i'd've\":\"I would have\",\n",
        " \"i'll\": \"I will\",\n",
        " \"i'm\": \"I am\",\n",
        " \"i'm'a\": \"I am about to\",\n",
        " \"i'm'o\": \"I am going to\",\n",
        " \"innit\": \"is it not\",\n",
        " \"i've\": \"I have\",\n",
        " \"isn't\": \"is not\",\n",
        " \"it'd\": \"it would\",\n",
        " \"it'll\": \"it will\",\n",
        " \"it's\": \"it is\",\n",
        " \"let's\": \"let us\", \n",
        " \"ma'am\": \"madam\",\n",
        " \"mayn't\": \"may not\",\n",
        " \"may've\": \"may have\",\n",
        " \"methinks\" : \"me thinks\",\n",
        " \"mightn't\": \"might not\",\n",
        " \"might've\": \"might have\",\n",
        " \"mustn't\": \"must not\",\n",
        " \"mustn't've\": \"must not have\",\n",
        " \"must've\": \"must have\",\n",
        " \"needn't\": \"need not\",\n",
        " \"ne'er\":\"never\",\n",
        " \"o'clock\": \"of the clock\",\n",
        " \"o'er\": \"over\",\n",
        " \"ol'\": \"old\",\n",
        " \"oughtn't\":\"ought not\",\n",
        " \"'s\": \"is\",\n",
        " \"shalln't\":\"shall not\",\n",
        " \"shan't\":\"shall not\",\n",
        " \"she'd\":\"she would\",\n",
        " \"she'll\":\"she will\",\n",
        " \"she's\":\"she is\",\n",
        " \"should've\":\"should have\",\n",
        " \"shouldn't\":\"should not\",\n",
        " \"shouldn't've\":\"should not have\",\n",
        " \"somebody's\":\"somebody is\",\n",
        " \"someone's\":\"someone is\",\n",
        " \"something's\":\"something is\",\n",
        " \"so're\":\"so you are\",\n",
        " \"that'll\":\"that will\",\n",
        " \"that're\":\"that are\",\n",
        " \"that's\":\"that is\",\n",
        " \"that'd\":\"that had\",\n",
        " \"there'd\":\"there would\",\n",
        " \"there'll\":\"here shall\",\n",
        " \"there're\":\"there are\",\n",
        " \"there's\":\"there has\",\n",
        " \"these're\":\"these are\",\n",
        " \"these've\":\"these have\",\n",
        " \"they'd\":\"they would\",\n",
        " \"they'll\":\"they will\",\n",
        " \"they're\":\"they are\",\n",
        " \"they've\":\"they have\",\n",
        " \"this's\":\"this is\",\n",
        " \"those're\":\"those are\",\n",
        " \"those've\":\"those have\",\n",
        " \"'tis\":\"it is\",\n",
        " \"to've\":\"to have\",\n",
        " \"'twas\":\"it was\",\n",
        " \"wanna\":\"want to\",\n",
        " \"wasn't\":\"was not\",\n",
        " \"we'd\":\"we would\",\n",
        " \"we'd've\":\"we would have\",\n",
        " \"we'll\":\"we will\",\n",
        " \"we're\":\"we are\",\n",
        " \"we've\":\"we have\",\n",
        " \"weren't\":\"were not\",\n",
        " \"what'd\":\"what did\",\n",
        " \"what'll\":\"what will\",\n",
        " \"what're\":\"what are\",\n",
        " \"what's\":\"what is\",\n",
        " \"what've\":\"what have\",\n",
        " \"when's\":\"when is\",\n",
        " \"where'd\":\"where did\",\n",
        " \"where'll\":\"where will\",\n",
        " \"where're\":\"where are\",\n",
        " \"where's\":\"where has\",\n",
        " \"where've\":\"where have\",\n",
        " \"which'd\":\"which had\",\n",
        " \"which'll\":\"which shall\",\n",
        " \"which're\":\"which are\",\n",
        " \"which's\":\"which has\",\n",
        " \"which've\":\"which have\",\n",
        " \"who'd\":\"who would\",\n",
        " \"who'd've\":\"who would have\",\n",
        " \"who'll\":\"who will\",\n",
        " \"who're\":\"who are\",\n",
        " \"who's\":\"who has\",\n",
        " \"who've\":\"who have\",\n",
        " \"why'd\":\"why did\",\n",
        " \"why're\":\"why are\",\n",
        " \"why's\":\"why is\",\n",
        " \"won't\":\"will not\",\n",
        " \"would've\":\"would have\",\n",
        " \"wouldn't\":\"would not\",\n",
        " \"wouldn't've\":\"would not have\",\n",
        " \"y'all\":\"you all\",\n",
        " \"y'all'd've\":\"you all would have\",\n",
        " \"y'all'dn't've'd\":\"you all would not have had\",\n",
        " \"y'all're\":\"you all are\",\n",
        " \"you'd\":\"you would\",\n",
        " \"you'll\":\"you will\",\n",
        " \"you're\":\"you are\",\n",
        " \"you've\":\"you have\",\n",
        "  \" u \" : \" you\",\n",
        " \" ur \" : \" your\",\n",
        " \" n \": \" and\",\n",
        " \"\\'ll\": \"will\",\n",
        " \"\\'ve\": \"have\",\n",
        " \"\\'re\": \"are\",\n",
        " \"\\'d\": \"would\",}\n",
        " \n",
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    if type(text) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            text = text.replace(key,value)\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        " \n",
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        " \n",
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "\n",
        "# Removing special charecters\n",
        "def remove_spl(text):\n",
        "    text = re.sub('[^A-Z a-z 0-9]+', \"\", text)\n",
        "    return text\n",
        " \n",
        "clean_questions_1 = []\n",
        "for question in clean_questions:\n",
        "    clean_questions_1.append(remove_spl(question))\n",
        "\n",
        "clean_answers_1 = []\n",
        "for answer in clean_answers:\n",
        "    clean_answers_1.append(remove_spl(answer))\n",
        "\n",
        "\n",
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions_1:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers_1:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        " \n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 20\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 20\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        " \n",
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        " \n",
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        " \n",
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers_1)):\n",
        "    clean_answers_1[i] += ' <EOS>'\n",
        " \n",
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions_1:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers_1:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)\n",
        " \n",
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEL8THzVsGzu",
        "colab_type": "text"
      },
      "source": [
        "### Reason for creating id2line dictionary\n",
        "\n",
        "- We have to obtain dataset which contains 2 columns:\n",
        "  1. Input: The input text that will be fed into the neural network\n",
        "  2. Output: The output will be the target or the answer received during the conversation by characters. Coz we will compare this target answers with the answers given by the chatbot to calculate the accuracy \n",
        "\n",
        "\n",
        "### The reason for creating list of conversations\n",
        "\n",
        "- The movie_conversations.txt file has to many metadata. Hence we only need to extract the last part i.e conversation with line_ids\n",
        "\n",
        "- Explanation for _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\" ' \", \"\").replace(\" \", \"\")\n",
        "\n",
        "- Split each line at +++$+++, take the last part of each line ie the conversations and then remove [] by selecting [1:-1] as it will remove the [ ] at 0th position and -1th position and finally replace '' and \" \" by nothing \"\".\n",
        "\n",
        "\n",
        "### Removing the Non frequent Words\n",
        "\n",
        "- Removing the non frequent words from the above lists\n",
        "- we do that to optimize our training as we need only the essential words in the dialogue\n",
        "- We remove the words that ouccur less than 5% of the times\n",
        "\n",
        "\n",
        "### Reason for using SOS, POD & EOS\n",
        "\n",
        "- SOS, PAD and EOS are added because the process training data and the sequences in the batches should all have the same length.Hence SOS and EOS are places in the empty positions. PAD is used to make the answers of same size \n",
        "- The first token that starts in the decoding layer is the SOS.\n",
        "- We will create a last token called OUT that corresponds to all the words that were filtered out as 5% least frequent words\n",
        "\n",
        "\n",
        "### Inverting answersword2int dictionary\n",
        "- The inverse dictionary of answersword2int dictionary is required for inverse mapping from integers to the answer words in our Seq2Seq model implementation\n",
        "\n",
        "\n",
        "### Reason for adding EOS at the end of every answer\n",
        "- Adding EOS at the end of every answer is very necessary at it is important to train the bot when to stop while answering.\n",
        "\n",
        "\n",
        "### Translating all the questions and answers to integers\n",
        "- We are doing this to sort all the questions and all the answers by their lengths to optimize training performance.\n",
        "\n",
        "\n",
        "### Sorting questions and answers by length of questions\n",
        "- We sort the questions and answers by length of questions in order to speed up the training and help to reduce the loss.\n",
        "- The reason for it is that it will reduce the amount of padding during the training\n",
        "- We will train the chatbot on short sentences and we will avoid the long sentences. This is because, when we teach a child how to talk, we make use of short sentences and not the long ones.\n",
        "- We use emumerate function to get the question and its index value as well. \n",
        "- We need index values to be added in sorted_clean_question and sorted_clean_answers list and we also need questions len and answer len for sorting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CteFhLuemhY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob\n",
        " \n",
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets\n",
        " \n",
        "# Creating the Encoder RNN\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    return encoder_state\n",
        " \n",
        "# Decoding the training set\n",
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)\n",
        " \n",
        "# Decoding the test/validation set\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions\n",
        " \n",
        "# Creating the Decoder RNN\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions\n",
        " \n",
        "# Building the seq2seq model\n",
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words + 1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
        "                                                         decoder_embeddings_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4zeCrq9u1f1",
        "colab_type": "text"
      },
      "source": [
        "### Creating placeholders for the inputs and the targets\n",
        "\n",
        "- All the variables used in tensors must be defined by a TF     placeholder. \n",
        "- Placeholders are advance datastructures that can hold tensors and other features as well\n",
        "\n",
        "### Preprocessing the targets\n",
        "\n",
        "- preprocessing of targets is necessary because the decoder will accept only a certain format of targets\n",
        "- The format of targets is 2 fold:\n",
        "    1. The targets have to be in batches (eg: batches of 10 answers are fed in the neural network)\n",
        "    2. Each answer in the batch must start with SOS token.\n",
        "- If we add SOS in the beginning of each answer, we have to remove the last word of each answer. We do this by using concatination\n",
        "\n",
        "\n",
        "### Creating the Encoder RNN\n",
        "\n",
        "- rnn_size is the number of input tensors of the encoder layer\n",
        "- encoder cell contains various LSTM layers\n",
        "- The bidirectional RNN function returns two values, encoder_state and encoder_output. Hence we use _, before encoder_state to specify that we only need the encoder state and not the first value\n",
        "\n",
        "\n",
        "- Embeddings: Embedding means converting the words into vector format\n",
        "- variable_scope: variable scope is a advanced data-structure that wraps the tensor flow variable \n",
        "\n",
        "### Decoding the training set\n",
        "\n",
        "- step 1: Initializing attention_states as a 3D matrix containing only zeros.\n",
        "- attention keys = keys to be compared with the target state.\n",
        "- attention values = Values that we will use to construct the context vectors (context is returned by encoder and used by decoder)\n",
        "- attention score = It is used to compare the similarities between keys and the target states.\n",
        "- attention construct = It is used to build the attention state.\n",
        "- We get decoder_output from dynamic_rnn_decoder. The dynamic_rnn_decoder gives out 3 values decoder_output, final_state and the final_context_state of the decoder. And hence we use _, _, as we only want decoder_output.\n",
        "\n",
        "\n",
        "### Decoding the test/validation set\n",
        "\n",
        "- for validation and test set we are using attention_decoder_fn_inference function\n",
        "- We are using this function because we want to deduce logically the answer to the question so that the chatbot can answer on his own based on the logic it has developed.\n",
        "- We are taking 4 new arguments [sos_id, eos_id, maximum_length, num_words] for attention_decoder_fn_infer function\n",
        "\n",
        "\n",
        "### Creating the Decoder RNN\n",
        "\n",
        "- We are making this decoding rnn layer in the decoding scope\n",
        "- Then we create the LSTM layer\n",
        "- for fully connected layer, x is the input, num_words ie the num of words in answer will be the output\n",
        "- the fully connected layer will take the features from the stacked lstm layers and will return the final scores\n",
        "- By using softmax, we will return the final answer\n",
        "- In test_predictions, we did sequence_length - 1 because to exclude the last token\n",
        "\n",
        "\n",
        "### Building the final Seq2Seq model\n",
        "\n",
        "- In building the seq2seq layer, we will assemble the encoder rnn and decoder rnn together\n",
        "- The function seq2seq model is the brain of our chatbot\n",
        "- questionsword2int dictionary is used to preprocess the targets\n",
        "- The function will return training predictions and test predictions\n",
        "- the encoder will give the encoder state and the decoder will give the training and test predictions\n",
        "- embed_sequence returns the embedded input of the encoder\n",
        "- encoder state is the output of the encoder and input to the decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLISawdDmrmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the hyperparameters\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "rnn_size = 512\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 512\n",
        "decoding_embedding_size = 512\n",
        "learning_rate = 0.01\n",
        "learning_rate_decay = 0.9\n",
        "minimum_learning_rate = 0.0001\n",
        "keep_probability = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "narsoSVM5arx",
        "colab_type": "text"
      },
      "source": [
        "### Setting the hyperparameters\n",
        "\n",
        "- The dropout is used to avoid overfitting. Dropout is present only during training the neural network. DUring test and validation, all the neurons are used.\n",
        "- The keep_prob (p) value is used to control the dropout rate p = 1 - dropout\n",
        "- According to paper by Geoffery Hinton, dropout of 20% in the input layer and 50% in the hidden layer is optimal.\n",
        "- learning_rate_decay = the rate at which the learning rate will reduce from 0.01 so that the model does indepth training\n",
        "- minimum_learning_rate = beacuse of the decay, the learning rate should not go below 0.0001\n",
        "- encoding_embedding_size = number of columns in your embedding matrix\n",
        "- decoding_embedding_size = number of columns in your decoding matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prWIOXHcm3Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a session\n",
        "\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlXCm5p65dsj",
        "colab_type": "text"
      },
      "source": [
        "### Defining a session\n",
        "\n",
        "- Here we have defined a tf session on which all the tf training will be run\n",
        "- For creating a session, we will first reset the tf graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXRZE3BRm6hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model inputs\n",
        "\n",
        "inputs, targets, lr, keep_prob = model_inputs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkBl_6W5v_u",
        "colab_type": "text"
      },
      "source": [
        "### Loading the model inputs\n",
        "\n",
        "- inputs = Questions that we will feed in the model\n",
        "- targets = Answers that we will receive for the questions\n",
        "- lr = learning rate\n",
        "- keep_prob = parameter to control the dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WChpIBY0hm-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
        " \n",
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjlq1VfJ52c3",
        "colab_type": "text"
      },
      "source": [
        "### Setting the Sequence length variable\n",
        "\n",
        "- arguments of sequence length:\n",
        "    1. input = 25 is the max length of the sequence\n",
        "    2. shape = tf shape ie list of integer which is set to None as the sequence_length is just a value and not a tensor.\n",
        "    3. name =  name of sequence length\n",
        "    \n",
        "- sequence_length = 25 means in training we will not use questions and answers with length more than 25\n",
        "\n",
        "### Getting the shape of input tensor\n",
        "\n",
        "- The shape of input tensor will be an argument for one specific function used for training\n",
        "- The function is called ones function by tf. and the dimension of ones function is set to input shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gkdoM4lhxeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtCK1hn66Hc2",
        "colab_type": "text"
      },
      "source": [
        "### Getting the training and Test predictions\n",
        "\n",
        "- Here we are trying to get the training and test predictions when we are loading the model inputs value in the neural network (see above)\n",
        "- The training predictions and test predictions are not the same as the local variables used in the seq2seq function.\n",
        "- These are real variables used in the training later on.\n",
        "- The inputs are not in the right shape. Hence we have to use reshape to shape the inputs\n",
        "- the tf.reverse function reverses the dimension of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOdYnOXZh1xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting up the loss error, the optimizer and gradient clipping\n",
        "\n",
        "with tf.name_scope(\"optimization\"):\n",
        "  loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions, targets, tf.ones([input_shape[0], sequence_length]))\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  gradients = optimizer.compute_gradients(loss_error)\n",
        "  clipped_gradients = [(tf.clip_by_value(grad_tensor, -5, 5), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "  optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWTDSqqvilKV",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the loss error, the optimizer and gradient clipping\n",
        "\n",
        "- Gradient clipping is a technique to keep the gradients in the graph between minimum value and the maximum value to avoid exploding and vanishing gradient issue\n",
        "- The loss error chosen is the weighted cross_entropy loss error as it is the best loss error for sequences\n",
        "- The loss error will be calculated between training_predictions and targets\n",
        "- The gradient in the graph is attached to a variable\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xln7SmdimZFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequence with <PAD> token\n",
        "\n",
        "def apply_padding(batch_of_sequences, word2count):\n",
        "  max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "  return [sequence + [word2count[\"<PAD>\"]] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEIcDO40ovrT",
        "colab_type": "text"
      },
      "source": [
        "### Padding the sequence with <PAD> token\n",
        "\n",
        "- We apply padding to the sequences with pad token\n",
        "- padding is done coz all the sentences in a batch whether they are questions or answers must have the same length\n",
        "- First we find the max length of the sequence present in the batch\n",
        "- Now we will use the max length of other sequences which are shorter\n",
        "- list of pad tokens is added to list of sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-DB0SNUsHxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into batches of questions and answers\n",
        "\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "  for batch_index in range (0, len(questions)//batch_size):\n",
        "    start_index = batch_index * batch_size\n",
        "    questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "    answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "    padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "    padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "    yield padded_questions_in_batch, padded_answers_in_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DtGAVgO7TXo",
        "colab_type": "text"
      },
      "source": [
        "### Split the data into batches of questions and answers\n",
        "\n",
        "\n",
        "- We have to create batches based on total number of questions\n",
        "- Hence we have to figure out what will be the total number of batches\n",
        "- Once the batch is formed, we will apply (apply_padding) so that all the questions and answers are of the same size.\n",
        "- We have to keep a track of which batch we are on. Hence we make use of batch_index\n",
        "- // is used coz we want an integer\n",
        "- start_index will be the index of the first question we are adding in the batch\n",
        "- for eg: batch_size = 64, batch_index = 0\n",
        "- Therefore, start_index = 0 * 64 hence first batch will have questions from 0-63\n",
        "- Next batch = 1 * 64 will have questions from 63 to 122\n",
        "- yield is similar to return but it performs better while dealing with sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlUbw9MpBqaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the questions and answers to training and validation set\n",
        "\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15) # integer value of 15% of sorted clean questions\n",
        "training_questions = sorted_clean_questions[training_validation_split :]\n",
        "training_answers = sorted_clean_answers[training_validation_split :]\n",
        "validation_questions = sorted_clean_questions[: training_validation_split]\n",
        "validation_answers = sorted_clean_answers[: training_validation_split]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Z1kfVxDTGb",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the questions and answers to training and validation set\n",
        "\n",
        "- The training set will be 85% of questions and answers and the testing sets will be 15% of the questions and answers\n",
        "- Training questions and answers will be all the questions and answers after training_validation_split\n",
        "- Validation questions and answers will be all the 15% questions and answers before the split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rZmVaVjDSTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 1000\n",
        "checkpoint = \"chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range (1, epochs + 1):\n",
        "  for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "    starting_time = time.time()\n",
        "    _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs : padded_questions_in_batch, \n",
        "                                                                                           targets: padded_answers_in_batch,\n",
        "                                                                                           lr : learning_rate,\n",
        "                                                                                           sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                           keep_prob: keep_probability})\n",
        "    total_training_loss_error += batch_training_loss_error\n",
        "    ending_time = time.time()\n",
        "    batch_time = ending_time - starting_time\n",
        "    if batch_index % batch_index_check_validation_loss == 0:\n",
        "      print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "      total_training_loss_error = 0\n",
        "    if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "      total_validation_loss_error = 0\n",
        "      starting_time = time.time()\n",
        "      for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "        batch_validation_loss_error = session.run(loss_error, {inputs : padded_questions_in_batch, \n",
        "                                                                  targets: padded_answers_in_batch,\n",
        "                                                                  lr : learning_rate,\n",
        "                                                                  sequence_length : padded_answers_in_batch.shape[1],\n",
        "                                                                  keep_prob : 1})\n",
        "        total_training_loss_error += batch_validation_loss_error\n",
        "      ending_time = time.time()\n",
        "      batch_time = ending_time - starting_time\n",
        "      average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "      print(\"Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds\".format(average_validation_loss_error, int(batch_time)))\n",
        "      learning_rate += learning_rate_decay\n",
        "      if learning_rate < minimum_learning_rate:\n",
        "        learning_rate = minimum_learning_rate\n",
        "      list_validation_loss_error.append(average_validation_loss_error)\n",
        "      if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "        print(\"I speak better now !!\")\n",
        "        early_stopping_check = 0\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(session, checkpoint)\n",
        "      else:\n",
        "        print(\"I do not speak better, I need to practice more.\")\n",
        "        early_stopping_check += 1\n",
        "        if early_stopping_check == early_stopping_stop:\n",
        "          break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "      print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "      break\n",
        "print(\"Game Over\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1mT2RjR06w",
        "colab_type": "text"
      },
      "source": [
        "### Training the chatbot\n",
        "\n",
        "\n",
        "- We will check the training loss per 100 batches.\n",
        "- The list_validation_loss_errors is used coz we will be using the early stopping technique which consists of checking if we have reached the min of all the losses that we get.\n",
        "- Whenever there is no change in the validation loss, the (early_stopping_check) will increase\n",
        "- If early_stopping_check reaches early_stopping_stop value then the entire operation will be stopped.\n",
        "- checkpoint = file containing the weights\n",
        "- First we iterate through each epoch and then in each epoch we iterate through each batch to get padded questions and answers\n",
        "- After starting_time, we will get the training loss error of the single batch\n",
        "- This training_loss_error will be added to total training loss error.\n",
        "- batch_time = training time of a single batch\n",
        "- We will now calculate the average of the training_loss_errors on 100 batches and we will print that error to keep track of training loss error \n",
        "- Next we calculate the average loss error for the validation set\n",
        "- We initialized total_validation_loss_error = 0\n",
        "- Since we are not doing any training with the validation, we remove the gradient_clipping_optimizer\n",
        "- Finally we will compute the average of validation losses\n",
        "- {:>6.3f} = float value with 6 figures and 3 decimals\n",
        "- Next we will apply decay to our learning rate\n",
        "- Next, we have to take care of early stopping\n",
        "- if average_validation_loss_error <= min(list_validation_loss_error): It means the error is reducing and there is an improvement and hence the chatbot will give the first sentence. Then we reset early stopping check to 0\n",
        "- Then we save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xG_5nhGjHN",
        "colab_type": "code",
        "outputId": "2d81e093-04d0-40ee-c98f-8b4f8cb5a103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loading the Weights and Running the session\n",
        "\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_14/tensor_names, save/RestoreV2_14/shape_and_slices)]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-66e0b7c800df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_14/tensor_names, save/RestoreV2_14/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_14', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-66e0b7c800df>\", line 6, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_14/tensor_names, save/RestoreV2_14/shape_and_slices)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hCsxeXcG9OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the questions from strings to list of Integers\n",
        "\n",
        "def convert_word2int(question, word2int):\n",
        "  question = clean_text(question)\n",
        "  return [word2int.get(word, word2int[\"<OUT>\"]) for word in question.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2G0rNmkuNSx",
        "colab_type": "text"
      },
      "source": [
        "### Converting the questions from strings to list of Integers\n",
        "\n",
        "- First we clean the questions\n",
        "- Return a list of integers after splitting the questions into words. And get the int value for out if the word is not in the word2int list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVbg6dbiuOVJ",
        "colab_type": "code",
        "outputId": "4bcbab4a-a1cb-442b-aebb-318994851972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Setting up the chat\n",
        "\n",
        "while(True):\n",
        "  question = input(\"You: \")\n",
        "  if question == \"Goodbye\" or question == \"Bye\":\n",
        "    print (\"Chatbot: Bye. Have a wonderful day!\")\n",
        "    break\n",
        "  question = convert_word2int(question, questionswords2int) # convert question words to integers\n",
        "  question = question + [questionswords2int[\"<PAD>\"]] * (20 - len(question)) # padding all the questions to the length of training questions\n",
        "  # putting the question in fake batch with one single questions and all other zeros as rnn accepts questions only in batches\n",
        "  fake_batch = np.zeros((batch_size, 20)) # all training questions have length 20\n",
        "  fake_batch[0] = question\n",
        "  predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0] # the predicted answer is the first element of the list hence we used [0] in the end\n",
        "  answer = \"\" # answer given by chatbot. We will replace i by I and <EOS> by full-stop. Once we have a full-stop, answer will break\n",
        "  for i in np.argmax(predicted_answer, 1):\n",
        "    if answersints2word[i] == \"i\":\n",
        "      token = \"I\"\n",
        "    elif answersints2word[i] == \"<EOS>\":\n",
        "      token = \".\"\n",
        "    elif answersints2word[i] == \"<OUT>\":\n",
        "      token = \"out\"\n",
        "    else:\n",
        "      token = \" \" + answersints2word[i]\n",
        "    answer += token\n",
        "    if token == \".\":\n",
        "      break\n",
        "  print(\"Chatbot: \" + answer)\n",
        "    \n",
        "     \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: Bye\n",
            "Chatbot: Bye. Have a wonderful day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gKSgu6s2tEw",
        "colab_type": "text"
      },
      "source": [
        "- Argmax takes the token_ids of the values in the predicted answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6UgjR6V626L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}